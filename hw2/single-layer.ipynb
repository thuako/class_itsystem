{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset_num = 100\n",
    "variance = 0.5\n",
    "wtflag = 0\n",
    "learning_rate = 0.4\n",
    "file_exist = False\n",
    "\n",
    "batch = 100\n",
    "epochs = 300\n",
    "\n",
    "\n",
    "'''learning_rate = 0.4\n",
    "file_exist = False\n",
    "\n",
    "batch = 100\n",
    "epochs = 300'''\n",
    "\n",
    "# making dataset\n",
    "\n",
    "d_set = np.empty((1, 3), dtype=float)\n",
    "if file_exist:\n",
    "    tmp = np.load(sys.argv[1])\n",
    "    d_set = np.append(d_set, tmp)\n",
    "else:\n",
    "    prob = rd.randint(0, 8, dataset_num)\n",
    "    for i in prob:\n",
    "        '''x = rd.normal(int(i/2)*2-1, variance)\n",
    "        y = rd.normal(int(i/4)*2-1, variance)\n",
    "        z = rd.normal(int(i/8)*2-1, variance)'''\n",
    "\n",
    "        '''x = rd.normal(int(i / 2) * 2 - 1, variance)\n",
    "        y = rd.normal(int(i / 4) * 2 - 1, variance)'''\n",
    "        x = rd.randint(0, 2)\n",
    "        y = rd.randint(0, 2)\n",
    "        rst = x | y\n",
    "        d_set = np.append(d_set, [[x, y, rst]], axis=0)\n",
    "    if wtflag == 1:\n",
    "        np.save('dataset.npy', d_set)\n",
    "d_set = np.delete(d_set, 0, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def SGD_single_Layer(x, w, node_num=1, bias=True, learning_rate = 0.01, ftn = 'sigmoid', mode='train' ):\n",
    "    # add bias\n",
    "    if bias == True:\n",
    "        x = np.hstack((np.ones((x.shape[0], 1)), x))\n",
    "        \n",
    "    #forwarding\n",
    "    a = np.dot(x[:, 0:-1] , w.T)\n",
    "    y_ = sigmoid(a)\n",
    "    E = np.sum((y_ + x[:, -1])**2) / d_set.shape[0]\n",
    "    \n",
    "    #backpropagation delta\n",
    "    delta = np.sum(y_ - x[:, -1]) / d_set.shape[0] *y_*(1-y_)\n",
    "    #delta * x\n",
    "    w_grad = np.array([x[i, 0:-1]*delta[i] for i in range(x.shape[0])])\n",
    "\n",
    "    correct = 0\n",
    "    #weight update\n",
    "    if mode is 'train':\n",
    "        w = w - np.sum(w_grad, axis=0)*learning_rate\n",
    "        return w, delta, E, y_\n",
    "    \n",
    "    #probable calculate\n",
    "    elif mode is 'test':\n",
    "        cnt = np.array(x[:, -1] < np.average(x[:, -1]))\n",
    "        correct = np.sum(cnt)/x.shape[0]*100\n",
    "        return w, correct, E, y_\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiple_Layer(x, w = [], num_of_hidden_node=5, bias=True, learning_rate = 0.01, ftn = 'sigmoid', mode='train' ):\n",
    "    # add bias\n",
    "    if bias == True:\n",
    "        x = np.hstack((np.ones((x.shape[0], 1)), x))\n",
    "        \n",
    "    #initialize weight\n",
    "    if w.size is 0:\n",
    "        w = np.array([rd.normal(0, 1, (x.shape[1]-1)*num_of_hidden_node).reshape],[rd.normal(0,1,num_of_hidden_node)])\n",
    "    \n",
    "        \n",
    "    #forwarding\n",
    "    a = np.dot(x[:, 0:-1] , w.T)\n",
    "    y_ = sigmoid(a)\n",
    "    E = np.sum((y_ - x[:, -1])**2) / d_set.shape[0]\n",
    "    \n",
    "    #backpropagation delta\n",
    "    delta = np.sum(y_ - x[:, -1]) / d_set.shape[0] *y_*(1-y_)\n",
    "    #delta * x\n",
    "    w_grad = np.array([x[i, 0:-1]*delta[i] for i in range(x.shape[0])])\n",
    "\n",
    "    correct = 0\n",
    "    #weight update\n",
    "    if mode is 'train':\n",
    "        w = w + np.sum(w_grad, axis=0)*learning_rate\n",
    "        return w, delta, E\n",
    "    \n",
    "    #probable calculate\n",
    "    elif mode is 'test':\n",
    "        cnt = np.array(np.abs(y_ - x[:, -1]) < 0.5)\n",
    "        print(cnt)\n",
    "        correct = np.sum(cnt)/x.shape[0]*100\n",
    "        return w, correct, E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.73666296 0.73666296 0.73666296 0.96630029 0.73736011 0.73666296\n",
      " 0.96630029 0.73736011 0.73736011 0.96630029 0.73666296 0.73666296\n",
      " 0.96618297 0.96618297 0.96618297 0.96618297 0.96618297 0.96630029\n",
      " 0.96630029 0.73736011 0.73736011 0.96630029 0.96630029 0.73666296\n",
      " 0.96630029 0.96618297 0.73736011 0.96618297 0.96630029 0.96630029\n",
      " 0.73736011 0.73736011 0.96630029 0.96630029 0.96618297 0.96618297\n",
      " 0.73736011 0.96618297 0.96618297 0.73666296 0.96630029 0.73736011\n",
      " 0.96630029 0.96630029 0.96618297 0.96618297 0.73736011 0.96618297\n",
      " 0.96618297 0.73666296 0.73736011 0.96630029 0.73736011 0.96618297\n",
      " 0.96630029 0.73736011 0.96618297 0.73666296 0.73666296 0.96618297\n",
      " 0.96630029 0.96618297 0.96618297 0.96630029 0.96630029 0.73666296\n",
      " 0.96630029 0.73736011 0.96630029 0.73666296 0.73736011 0.96630029\n",
      " 0.96618297 0.96618297 0.73666296 0.96618297 0.73736011 0.96630029\n",
      " 0.73666296 0.73736011 0.73736011 0.96618297 0.96630029 0.96618297\n",
      " 0.73736011 0.73736011 0.96630029 0.73736011 0.73736011 0.73666296\n",
      " 0.96630029 0.73666296 0.73736011 0.96630029 0.73666296 0.96618297\n",
      " 0.96618297 0.96630029 0.96630029 0.96618297 0.96630029]\n",
      "\n",
      "\n",
      "2.5253 : train cost, 3.1031 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.69595695 0.69595695 0.69595695 0.95280473 0.67244164 0.69595695\n",
      " 0.95280473 0.67244164 0.67244164 0.95280473 0.69595695 0.69595695\n",
      " 0.95746597 0.95746597 0.95746597 0.95746597 0.95746597 0.95280473\n",
      " 0.95280473 0.67244164 0.67244164 0.95280473 0.95280473 0.69595695\n",
      " 0.95280473 0.95746597 0.67244164 0.95746597 0.95280473 0.95280473\n",
      " 0.67244164 0.67244164 0.95280473 0.95280473 0.95746597 0.95746597\n",
      " 0.67244164 0.95746597 0.95746597 0.69595695 0.95280473 0.67244164\n",
      " 0.95280473 0.95280473 0.95746597 0.95746597 0.67244164 0.95746597\n",
      " 0.95746597 0.69595695 0.67244164 0.95280473 0.67244164 0.95746597\n",
      " 0.95280473 0.67244164 0.95746597 0.69595695 0.69595695 0.95746597\n",
      " 0.95280473 0.95746597 0.95746597 0.95280473 0.95280473 0.69595695\n",
      " 0.95280473 0.67244164 0.95280473 0.69595695 0.67244164 0.95280473\n",
      " 0.95746597 0.95746597 0.69595695 0.95746597 0.67244164 0.95280473\n",
      " 0.69595695 0.67244164 0.67244164 0.95746597 0.95280473 0.95746597\n",
      " 0.67244164 0.67244164 0.95280473 0.67244164 0.67244164 0.69595695\n",
      " 0.95280473 0.69595695 0.67244164 0.95280473 0.69595695 0.95746597\n",
      " 0.95746597 0.95280473 0.95280473 0.95746597 0.69595695]\n",
      "\n",
      "\n",
      "3.0644 : train cost, 2.9802 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.67539927 0.67539927 0.67539927 0.94451476 0.63857612 0.67539927\n",
      " 0.94451476 0.63857612 0.63857612 0.94451476 0.67539927 0.67539927\n",
      " 0.95248699 0.95248699 0.95248699 0.95248699 0.95248699 0.94451476\n",
      " 0.94451476 0.63857612 0.63857612 0.94451476 0.94451476 0.67539927\n",
      " 0.94451476 0.95248699 0.63857612 0.95248699 0.94451476 0.94451476\n",
      " 0.63857612 0.63857612 0.94451476 0.94451476 0.95248699 0.95248699\n",
      " 0.63857612 0.95248699 0.95248699 0.67539927 0.94451476 0.63857612\n",
      " 0.94451476 0.94451476 0.95248699 0.95248699 0.63857612 0.95248699\n",
      " 0.95248699 0.67539927 0.63857612 0.94451476 0.63857612 0.95248699\n",
      " 0.94451476 0.63857612 0.95248699 0.67539927 0.67539927 0.95248699\n",
      " 0.94451476 0.95248699 0.95248699 0.94451476 0.94451476 0.67539927\n",
      " 0.94451476 0.63857612 0.94451476 0.67539927 0.63857612 0.94451476\n",
      " 0.95248699 0.95248699 0.67539927 0.95248699 0.63857612 0.94451476\n",
      " 0.67539927 0.63857612 0.63857612 0.95248699 0.94451476 0.95248699\n",
      " 0.63857612 0.63857612 0.94451476 0.63857612 0.63857612 0.67539927\n",
      " 0.94451476 0.67539927 0.63857612 0.94451476 0.67539927 0.95248699\n",
      " 0.95248699 0.94451476 0.94451476 0.95248699 0.67539927]\n",
      "\n",
      "\n",
      "2.9754 : train cost, 2.9328 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.67010185 0.67010185 0.67010185 0.94219105 0.62977301 0.67010185\n",
      " 0.94219105 0.62977301 0.62977301 0.94219105 0.67010185 0.67010185\n",
      " 0.95112907 0.95112907 0.95112907 0.95112907 0.95112907 0.94219105\n",
      " 0.94219105 0.62977301 0.62977301 0.94219105 0.94219105 0.67010185\n",
      " 0.94219105 0.95112907 0.62977301 0.95112907 0.94219105 0.94219105\n",
      " 0.62977301 0.62977301 0.94219105 0.94219105 0.95112907 0.95112907\n",
      " 0.62977301 0.95112907 0.95112907 0.67010185 0.94219105 0.62977301\n",
      " 0.94219105 0.94219105 0.95112907 0.95112907 0.62977301 0.95112907\n",
      " 0.95112907 0.67010185 0.62977301 0.94219105 0.62977301 0.95112907\n",
      " 0.94219105 0.62977301 0.95112907 0.67010185 0.67010185 0.95112907\n",
      " 0.94219105 0.95112907 0.95112907 0.94219105 0.94219105 0.67010185\n",
      " 0.94219105 0.62977301 0.94219105 0.67010185 0.62977301 0.94219105\n",
      " 0.95112907 0.95112907 0.67010185 0.95112907 0.62977301 0.94219105\n",
      " 0.67010185 0.62977301 0.62977301 0.95112907 0.94219105 0.95112907\n",
      " 0.62977301 0.62977301 0.94219105 0.62977301 0.62977301 0.67010185\n",
      " 0.94219105 0.67010185 0.62977301 0.94219105 0.67010185 0.95112907\n",
      " 0.95112907 0.94219105 0.94219105 0.95112907 0.95112907]\n",
      "\n",
      "\n",
      "2.9283 : train cost, 2.9539 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66928568 0.66928568 0.66928568 0.94182572 0.62841456 0.66928568\n",
      " 0.94182572 0.62841456 0.62841456 0.94182572 0.66928568 0.66928568\n",
      " 0.95091689 0.95091689 0.95091689 0.95091689 0.95091689 0.94182572\n",
      " 0.94182572 0.62841456 0.62841456 0.94182572 0.94182572 0.66928568\n",
      " 0.94182572 0.95091689 0.62841456 0.95091689 0.94182572 0.94182572\n",
      " 0.62841456 0.62841456 0.94182572 0.94182572 0.95091689 0.95091689\n",
      " 0.62841456 0.95091689 0.95091689 0.66928568 0.94182572 0.62841456\n",
      " 0.94182572 0.94182572 0.95091689 0.95091689 0.62841456 0.95091689\n",
      " 0.95091689 0.66928568 0.62841456 0.94182572 0.62841456 0.95091689\n",
      " 0.94182572 0.62841456 0.95091689 0.66928568 0.66928568 0.95091689\n",
      " 0.94182572 0.95091689 0.95091689 0.94182572 0.94182572 0.66928568\n",
      " 0.94182572 0.62841456 0.94182572 0.66928568 0.62841456 0.94182572\n",
      " 0.95091689 0.95091689 0.66928568 0.95091689 0.62841456 0.94182572\n",
      " 0.66928568 0.62841456 0.62841456 0.95091689 0.94182572 0.95091689\n",
      " 0.62841456 0.62841456 0.94182572 0.62841456 0.62841456 0.66928568\n",
      " 0.94182572 0.66928568 0.62841456 0.94182572 0.66928568 0.95091689\n",
      " 0.95091689 0.94182572 0.94182572 0.95091689 0.62841456]\n",
      "\n",
      "\n",
      "2.9159 : train cost, 2.9405 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66917981 0.66917981 0.66917981 0.94177818 0.6282383  0.66917981\n",
      " 0.94177818 0.6282383  0.6282383  0.94177818 0.66917981 0.66917981\n",
      " 0.95088931 0.95088931 0.95088931 0.95088931 0.95088931 0.94177818\n",
      " 0.94177818 0.6282383  0.6282383  0.94177818 0.94177818 0.66917981\n",
      " 0.94177818 0.95088931 0.6282383  0.95088931 0.94177818 0.94177818\n",
      " 0.6282383  0.6282383  0.94177818 0.94177818 0.95088931 0.95088931\n",
      " 0.6282383  0.95088931 0.95088931 0.66917981 0.94177818 0.6282383\n",
      " 0.94177818 0.94177818 0.95088931 0.95088931 0.6282383  0.95088931\n",
      " 0.95088931 0.66917981 0.6282383  0.94177818 0.6282383  0.95088931\n",
      " 0.94177818 0.6282383  0.95088931 0.66917981 0.66917981 0.95088931\n",
      " 0.94177818 0.95088931 0.95088931 0.94177818 0.94177818 0.66917981\n",
      " 0.94177818 0.6282383  0.94177818 0.66917981 0.6282383  0.94177818\n",
      " 0.95088931 0.95088931 0.66917981 0.95088931 0.6282383  0.94177818\n",
      " 0.66917981 0.6282383  0.6282383  0.95088931 0.94177818 0.95088931\n",
      " 0.6282383  0.6282383  0.94177818 0.6282383  0.6282383  0.66917981\n",
      " 0.94177818 0.66917981 0.6282383  0.94177818 0.66917981 0.95088931\n",
      " 0.95088931 0.94177818 0.94177818 0.95088931 0.95088931]\n",
      "\n",
      "\n",
      "2.914 : train cost, 2.9518 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916646 0.66916646 0.66916646 0.94177218 0.62821608 0.66916646\n",
      " 0.94177218 0.62821608 0.62821608 0.94177218 0.66916646 0.66916646\n",
      " 0.95088583 0.95088583 0.95088583 0.95088583 0.95088583 0.94177218\n",
      " 0.94177218 0.62821608 0.62821608 0.94177218 0.94177218 0.66916646\n",
      " 0.94177218 0.95088583 0.62821608 0.95088583 0.94177218 0.94177218\n",
      " 0.62821608 0.62821608 0.94177218 0.94177218 0.95088583 0.95088583\n",
      " 0.62821608 0.95088583 0.95088583 0.66916646 0.94177218 0.62821608\n",
      " 0.94177218 0.94177218 0.95088583 0.95088583 0.62821608 0.95088583\n",
      " 0.95088583 0.66916646 0.62821608 0.94177218 0.62821608 0.95088583\n",
      " 0.94177218 0.62821608 0.95088583 0.66916646 0.66916646 0.95088583\n",
      " 0.94177218 0.95088583 0.95088583 0.94177218 0.94177218 0.66916646\n",
      " 0.94177218 0.62821608 0.94177218 0.66916646 0.62821608 0.94177218\n",
      " 0.95088583 0.95088583 0.66916646 0.95088583 0.62821608 0.94177218\n",
      " 0.66916646 0.62821608 0.62821608 0.95088583 0.94177218 0.95088583\n",
      " 0.62821608 0.62821608 0.94177218 0.62821608 0.62821608 0.66916646\n",
      " 0.94177218 0.66916646 0.62821608 0.94177218 0.66916646 0.95088583\n",
      " 0.95088583 0.94177218 0.94177218 0.95088583 0.66916646]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9182 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916479 0.66916479 0.66916479 0.94177143 0.62821329 0.66916479\n",
      " 0.94177143 0.62821329 0.62821329 0.94177143 0.66916479 0.66916479\n",
      " 0.95088539 0.95088539 0.95088539 0.95088539 0.95088539 0.94177143\n",
      " 0.94177143 0.62821329 0.62821329 0.94177143 0.94177143 0.66916479\n",
      " 0.94177143 0.95088539 0.62821329 0.95088539 0.94177143 0.94177143\n",
      " 0.62821329 0.62821329 0.94177143 0.94177143 0.95088539 0.95088539\n",
      " 0.62821329 0.95088539 0.95088539 0.66916479 0.94177143 0.62821329\n",
      " 0.94177143 0.94177143 0.95088539 0.95088539 0.62821329 0.95088539\n",
      " 0.95088539 0.66916479 0.62821329 0.94177143 0.62821329 0.95088539\n",
      " 0.94177143 0.62821329 0.95088539 0.66916479 0.66916479 0.95088539\n",
      " 0.94177143 0.95088539 0.95088539 0.94177143 0.94177143 0.66916479\n",
      " 0.94177143 0.62821329 0.94177143 0.66916479 0.62821329 0.94177143\n",
      " 0.95088539 0.95088539 0.66916479 0.95088539 0.62821329 0.94177143\n",
      " 0.66916479 0.62821329 0.62821329 0.95088539 0.94177143 0.95088539\n",
      " 0.62821329 0.62821329 0.94177143 0.62821329 0.62821329 0.66916479\n",
      " 0.94177143 0.66916479 0.62821329 0.94177143 0.66916479 0.95088539\n",
      " 0.95088539 0.94177143 0.94177143 0.95088539 0.94177143]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916458 0.66916458 0.66916458 0.94177134 0.62821294 0.66916458\n",
      " 0.94177134 0.62821294 0.62821294 0.94177134 0.66916458 0.66916458\n",
      " 0.95088534 0.95088534 0.95088534 0.95088534 0.95088534 0.94177134\n",
      " 0.94177134 0.62821294 0.62821294 0.94177134 0.94177134 0.66916458\n",
      " 0.94177134 0.95088534 0.62821294 0.95088534 0.94177134 0.94177134\n",
      " 0.62821294 0.62821294 0.94177134 0.94177134 0.95088534 0.95088534\n",
      " 0.62821294 0.95088534 0.95088534 0.66916458 0.94177134 0.62821294\n",
      " 0.94177134 0.94177134 0.95088534 0.95088534 0.62821294 0.95088534\n",
      " 0.95088534 0.66916458 0.62821294 0.94177134 0.62821294 0.95088534\n",
      " 0.94177134 0.62821294 0.95088534 0.66916458 0.66916458 0.95088534\n",
      " 0.94177134 0.95088534 0.95088534 0.94177134 0.94177134 0.66916458\n",
      " 0.94177134 0.62821294 0.94177134 0.66916458 0.62821294 0.94177134\n",
      " 0.95088534 0.95088534 0.66916458 0.95088534 0.62821294 0.94177134\n",
      " 0.66916458 0.62821294 0.62821294 0.95088534 0.94177134 0.95088534\n",
      " 0.62821294 0.62821294 0.94177134 0.62821294 0.62821294 0.66916458\n",
      " 0.94177134 0.66916458 0.62821294 0.94177134 0.66916458 0.95088534\n",
      " 0.95088534 0.94177134 0.94177134 0.95088534 0.94177134]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.94177132]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9514 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 1. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.95088533]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9517 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[0. 0. 0.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.66916455]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9181 : test cost, 18.81188118811881 : correction\n",
      "data y : \n",
      "[1. 0. 1.]\n",
      " prediction y : \n",
      "[0.66916455 0.66916455 0.66916455 0.94177132 0.62821289 0.66916455\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.66916455 0.66916455\n",
      " 0.95088533 0.95088533 0.95088533 0.95088533 0.95088533 0.94177132\n",
      " 0.94177132 0.62821289 0.62821289 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.95088533 0.62821289 0.95088533 0.94177132 0.94177132\n",
      " 0.62821289 0.62821289 0.94177132 0.94177132 0.95088533 0.95088533\n",
      " 0.62821289 0.95088533 0.95088533 0.66916455 0.94177132 0.62821289\n",
      " 0.94177132 0.94177132 0.95088533 0.95088533 0.62821289 0.95088533\n",
      " 0.95088533 0.66916455 0.62821289 0.94177132 0.62821289 0.95088533\n",
      " 0.94177132 0.62821289 0.95088533 0.66916455 0.66916455 0.95088533\n",
      " 0.94177132 0.95088533 0.95088533 0.94177132 0.94177132 0.66916455\n",
      " 0.94177132 0.62821289 0.94177132 0.66916455 0.62821289 0.94177132\n",
      " 0.95088533 0.95088533 0.66916455 0.95088533 0.62821289 0.94177132\n",
      " 0.66916455 0.62821289 0.62821289 0.95088533 0.94177132 0.95088533\n",
      " 0.62821289 0.62821289 0.94177132 0.62821289 0.62821289 0.66916455\n",
      " 0.94177132 0.66916455 0.62821289 0.94177132 0.66916455 0.95088533\n",
      " 0.95088533 0.94177132 0.94177132 0.95088533 0.62821289]\n",
      "\n",
      "\n",
      "2.9137 : train cost, 2.9402 : test cost, 17.82178217821782 : correction\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train(x):\n",
    "    # initialize weight\n",
    "    train_hist_E = []\n",
    "    test_hist_E = []\n",
    "    correct_hist = []\n",
    "    w_hist = []\n",
    "    test_y_hist = []\n",
    "    \n",
    "    b = rd.normal(1, 1, 3)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        #train run!\n",
    "        b, delta, E, y_ = SGD_single_Layer(d_set, b, learning_rate = learning_rate)\n",
    "        train_hist_E.append(E)\n",
    "        w_hist.append(b)\n",
    "        \n",
    "        #making test_set\n",
    "        test_set = np.empty((1, 3))\n",
    "        prob = rd.randint(0, 8, dataset_num)\n",
    "        for i in prob:\n",
    "            '''x = rd.normal(int(i/2)*2-1, variance)\n",
    "            y = rd.normal(int(i/4)*2-1, variance)\n",
    "            z = rd.normal(int(i/8)*2-1, variance)'''\n",
    "            x = rd.randint(0, 2)\n",
    "            y = rd.randint(0, 2)\n",
    "            rst = x | y\n",
    "            test_set = np.append(d_set, [[x, y, rst]], axis=0)\n",
    "\n",
    "        #test run!\n",
    "        test_b, correction, test_E, y_ =SGD_single_Layer(test_set, b, mode='test')\n",
    "        test_hist_E.append(test_E)\n",
    "        correct_hist.append(correction)\n",
    "        test_y_hist.append(y_)\n",
    "        \n",
    "        print(f\"data y : \\n{test_set[-1]}\\n prediction y : \\n{y_}\\n\\n\")\n",
    "        #print\n",
    "        print(f'{E:.5} : train cost, {test_E:.5} : test cost, {correction} : correction')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(train_hist_E, 'b')\n",
    "    plt.plot(test_hist_E, 'r')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(correct_hist)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(w_hist)\n",
    "    plt.show()\n",
    "train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
