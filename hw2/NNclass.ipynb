{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5e032f6fcb9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_x_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_y_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-5e032f6fcb9e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, train_set_x, train_set_y, test_set_x, test_set_y, feature, learning_rate, activate)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# w initialization sigmoid use xavier initialization  Relu using He initialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mactivate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_set_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rd\n",
    "import sys\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dataset_num = 4\n",
    "variance = 0.5\n",
    "wtflag = 0\n",
    "learning_rate = 0.3\n",
    "file_exist = False\n",
    "\n",
    "batch = 100\n",
    "epochs = 100\n",
    "\n",
    "# making dataset\n",
    "\n",
    "\n",
    "def making_data_set():\n",
    "    d_set = np.empty((1, 3), dtype=float)\n",
    "    if file_exist:\n",
    "        tmp = np.load(sys.argv[1])\n",
    "        d_set = np.append(d_set, tmp)\n",
    "    else:\n",
    "        prob = rd.randint(0, 8, dataset_num)\n",
    "        for i in prob:\n",
    "            '''x = rd.normal(int(i/2)*2-1, variance)\n",
    "            y = rd.normal(int(i/4)*2-1, variance)\n",
    "            z = rd.normal(int(i/8)*2-1, variance)'''\n",
    "\n",
    "            '''x = rd.normal(int(i / 2) * 2 - 1, variance)\n",
    "            y = rd.normal(int(i / 4) * 2 - 1, variance)'''\n",
    "            x = rd.randint(0, 2)\n",
    "            y = rd.randint(0, 2)\n",
    "            rst = x | y\n",
    "            d_set = np.append(d_set, [[x, y, rst]], axis=0)\n",
    "        if wtflag == 1:\n",
    "            np.save('dataset.npy', d_set)\n",
    "    d_set = np.delete(d_set, 0, 0)\n",
    "    return d_set\n",
    "\n",
    "t_data = making_data_set()\n",
    "t_x_data = t_data[:, 0:-1]\n",
    "t_y_data = t_data[:, -1].reshape(-1, 1)\n",
    "\n",
    "test_data = making_data_set()\n",
    "test_x_data = test_data[:, 0:-1]\n",
    "test_y_data = test_data[:, -1].reshape(-1, 1)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "class layer_class:\n",
    "    w = np.array([]); bias = True;  dataflow = np.array([]);  delta = np.array([]);  y_ = 0\n",
    "    def __init__(self, w, bias=False):\n",
    "        self.w = w; self.bias = bias\n",
    "\n",
    "\n",
    "class NN:\n",
    "    train_set_x, train_set_y, test_set_x, test_set_y, learning_rate,  activate = np.array([]),np.array([]),np.array([]),np.array([]), 0, 'sigmoid'\n",
    "    layer, train_loss, test_loss, accuracy = [], [], [], []\n",
    "    feature = []\n",
    "    def __init__(self, train_set_x, train_set_y, test_set_x, test_set_y, feature, learning_rate=0.01, activate='sigmoid'):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activate = activate\n",
    "        self.train_set_x = train_set_x\n",
    "        self.train_set_y = train_set_y\n",
    "        self.test_set_x = test_set_x\n",
    "        self.test_set_y = test_set_y\n",
    "        self.feature = feature\n",
    "        # w initialization sigmoid use xavier initialization  Relu using He initialization\n",
    "        if activate is 'sigmoid':\n",
    "            self.feature.insert(0, [self.train_set_x.shape[1], self.feature[0][1], self.feature[0][2]])\n",
    "            for i, f in enumerate(self.feature):\n",
    "                if i is (len(self.feature)-1):\n",
    "                    break\n",
    "                w = 0\n",
    "                \n",
    "                if f[2] is 'sigmoid':\n",
    "                    w = rd.randn(f[0], int(self.feature[i + 1][0])) / np.sqrt(1/f[0])\n",
    "                elif f[2] is 'relu':\n",
    "                    w = rd.randn(f[0], int(self.feature[i + 1][0])) / np.sqrt(2/f[0])\n",
    "                    \n",
    "                if f[1] is True:\n",
    "                    w = np.vstack(w, np.zeros((1, int(self.feature[i + 1][0]))))\n",
    "                self.layer.append(layer_class(w))\n",
    "            w = 0\n",
    "            if self.feature[-1][2] is 'sigmoid':\n",
    "                w = rd.randn(int(self.feature[-1][0]), self.train_set_y.shape[1]) / np.sqrt(1 / self.feature[-1][0])\n",
    "            elif self.feature[-1][2] is 'relu':\n",
    "                w = rd.randn(int(self.feature[-1][0]), self.train_set_y.shape[1]) / np.sqrt(2 / self.feature[-1][0])\n",
    "            self.layer.append(layer_class(w))\n",
    "            self.layer[0].dataflow = train_set_x\n",
    "\n",
    "\n",
    "    def sigmoid(self, a):\n",
    "        return a * (1 - a)\n",
    "    def relu(self, a):\n",
    "        a = a if a > 0 else 0\n",
    "        return a\n",
    "    def dsigmoid(self, a):\n",
    "        return a * (1 - a)\n",
    "    def drelu(self, a):\n",
    "        if a > 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # forwarding\n",
    "    def pri(self):\n",
    "        for i in self.layer:\n",
    "            print(i.w)\n",
    "        print(\"\\n\", self.layer[0].dataflow)\n",
    "\n",
    "    def forwarding(self):\n",
    "        ftn = 0\n",
    "        if self.activate == 'sigmoid':\n",
    "            ftn = self.sigmoid\n",
    "        elif self.activate == 'relu':\n",
    "            ftn = self.relu\n",
    "\n",
    "        for i, k in enumerate(self.layer):\n",
    "            a = np.array([])\n",
    "            if i is 0:\n",
    "                a = np.append(a, np.dot(self.train_set_x, k.w))\n",
    "            else:\n",
    "                a = np.append(a, np.dot(self.layer[i-1].dataflow, k.w))\n",
    "            a = a.reshape(self.train_set_x.shape[0], k.w.shape[1])\n",
    "            k.dataflow = ftn(a)\n",
    "        self.train_loss = np.append(self.train_loss, np.average((self.layer[-1].dataflow - self.train_set_y) ** 2))\n",
    "\n",
    "        for i, k in enumerate(self.layer):\n",
    "            a = np.array([])\n",
    "            if i is 0:\n",
    "                a = np.append(a, np.dot(self.test_set_x, k.w))\n",
    "            else:\n",
    "                a = np.append(a, np.dot(self.layer[i-1].dataflow, k.w))\n",
    "            a = a.reshape(self.test_set_x.shape[0], k.w.shape[1])\n",
    "            k.dataflow = ftn(a)\n",
    "        self.test_loss = np.append(self.test_loss, np.average((self.layer[-1].dataflow - self.train_set_y) ** 2))\n",
    "\n",
    "\n",
    "    def backpropagation(self):\n",
    "        dftn = 0\n",
    "        if self.activate == 'sigmoid':\n",
    "            dftn = self.dsigmoid\n",
    "        elif self.activate == 'relu':\n",
    "            dftn = self.drelu\n",
    "\n",
    "\n",
    "        for i, L in reversed(list(enumerate(self.layer))):\n",
    "            '''summation {}{j} wij delatj 교재 slide 12p'''\n",
    "\n",
    "            # fully connected layer\n",
    "            if i is (len(self.layer)-1):\n",
    "                L.delta = np.multiply((self.train_set_y - L.dataflow), dftn(L.dataflow))\n",
    "                #print(f\"$$$$$$$$$i :{i}, L.dataflow : {L.dataflow.shape} L.delta : {L.delta.shape}, L.w : {L.w.shape}\")\n",
    "            #else\n",
    "            else:\n",
    "                back_L = self.layer[i+1]\n",
    "                L.delta = np.multiply(dftn(L.dataflow), np.dot(back_L.delta, back_L.w.T))\n",
    "                # print(f\"L.delta 구하는 과정\\n\\ni :{i}, L.dataflow : {L.dataflow.shape} back_L.delta : {back_L.delta.shape}, back_L.w : {back_L.w.shape}, L.w. : {L.w.shape}\\n 구하기 end \\n\\n\")\n",
    "\n",
    "            # i가 0 이면 input data가 train_set\n",
    "            if i is 0 :\n",
    "                input = self.train_set_x\n",
    "            else:\n",
    "                input = self.layer[i - 1].dataflow\n",
    "\n",
    "            '''x_k * delta -> upgrade할 weight '''\n",
    "            upgrade_w = np.zeros(1)\n",
    "            #outer로 한꺼번에 추가후에, reshape로 정렬\n",
    "            for k in range(L.delta.shape[0]):\n",
    "                upgrade_w = np.append(upgrade_w, np.outer(input[k], L.delta[k]))\n",
    "                #print(f\" for {i} layer **********************\\ndelta : \\n{L.delta[k]}\\n input : \\n{input[k]}\\n result : \\n{np.outer(input[k], L.delta[k])}\")\n",
    "            upgrade_w = np.delete(upgrade_w, 0, axis=0)\n",
    "            upgrade_w = upgrade_w.reshape(-1, L.w.shape[0], L.w.shape[1])\n",
    "\n",
    "            \"\"\"update weight\"\"\"\n",
    "            L.w = L.w - np.average(upgrade_w, axis=0) * self.learning_rate\n",
    "\n",
    "    def training(self, epochs):\n",
    "        for i in range(epochs):\n",
    "            self.forwarding()\n",
    "            self.backpropagation()\n",
    "\n",
    "            cnt = np.array(np.abs(test_y_data - self.layer[-1].dataflow) < 0.5)\n",
    "            correct = np.average(cnt) * 100\n",
    "            self.accuracy.append(correct)\n",
    "            print(f\"epochs : {i}\\t train_loss :{self.train_loss[i]:.5}\\t test_loss :{self.test_loss[i]:.5}, accuracy :{correct}\")\n",
    "\n",
    "    def plot(self):\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(self.train_loss, 'b')\n",
    "        plt.plot(self.test_loss, 'y')\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.plot(self.accuracy)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test = NN(t_x_data, t_y_data, test_x_data, test_y_data, [3, 5, 6], learning_rate=learning_rate)\n",
    "\n",
    "test.training(epochs)\n",
    "test.plot()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
